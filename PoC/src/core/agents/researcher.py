# src/core/agents/researcher.py

from typing import List, Dict
import re
from .. import schemas
from ..tools.file_search import markdown_search_tool
from ..tools.web_search import web_search_tool

def research_agent_node(state: schemas.GraphState) -> Dict[str, schemas.ResearchReport]:
    """
    ãƒªã‚µãƒ¼ãƒãƒ£ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹LangGraphã®ãƒãƒ¼ãƒ‰ã€‚
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã«åŸºã¥ã„ã¦å†…éƒ¨æ–‡æ›¸ã¨ã‚¦ã‚§ãƒ–ã‚’æ¤œç´¢ã—ã€çµæœã‚’çµ±åˆã—ã¦
    ä¸€ã¤ã®ResearchReportã‚’ä½œæˆã™ã‚‹ã€‚

    Args:
        state: ç¾åœ¨ã®GraphStateã€‚'initial_user_request' ã‚’ä½¿ç”¨ã™ã‚‹ã€‚

    Returns:
        æ›´æ–°ã•ã‚ŒãŸStateã®ä¸€éƒ¨ã€‚'research_report' ã‚­ãƒ¼ã‚’å«ã‚€è¾æ›¸ã€‚
    """
    print("--- ğŸ•µï¸ ãƒªã‚µãƒ¼ãƒãƒ£ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œä¸­... ---")
    
    query = state.get("initial_user_request")
    if not query:
        # ã“ã®çŠ¶æ³ã¯é€šå¸¸ç™ºç”Ÿã—ãªã„ãŒã€å®‰å…¨ã®ãŸã‚ã®ã‚¬ãƒ¼ãƒ‰ç¯€
        print("  [è­¦å‘Š] ãƒªã‚µãƒ¼ãƒã‚¯ã‚¨ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒªã‚µãƒ¼ãƒã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return {}

    # --- è²¬å‹™ã®åˆ†é›¢: å„ãƒ„ãƒ¼ãƒ«ã‚’ç‹¬ç«‹ã—ã¦å‘¼ã³å‡ºã™ ---
    # 1. å†…éƒ¨æ–‡æ›¸ã®æ¤œç´¢ï¼ˆå…¨ä½“ï¼‰
    internal_findings = markdown_search_tool.search(query)
    print(f"  å†…éƒ¨æ–‡æ›¸ã‹ã‚‰ {len(internal_findings)} ä»¶ã®æƒ…å ±ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚")

    # 2. ã‚¦ã‚§ãƒ–ã®æ¤œç´¢
    web_findings = web_search_tool.search(query)
    print(f"  ã‚¦ã‚§ãƒ–ã‹ã‚‰ {len(web_findings)} ä»¶ã®æƒ…å ±ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚")
    
    # 3. çµæœã®çµ±åˆ
    all_findings = internal_findings + web_findings
    if not all_findings:
        print("  [æƒ…å ±] é–¢é€£ã™ã‚‹æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        # ç©ºã®ãƒ¬ãƒãƒ¼ãƒˆã§ã‚‚ã€å¾Œç¶šã®å‡¦ç†ã®ãŸã‚ã«ã‚¹ã‚­ãƒ¼ãƒã«æ²¿ã£ãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™
        report = schemas.ResearchReport(findings=[], summary="é–¢é€£æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        # ã“ã“ã§LLMã‚’å‘¼ã³å‡ºã—ã€ç™ºè¦‹ã—ãŸå…¨æƒ…å ±ã‹ã‚‰è¦ç´„ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ï¼ˆå°†æ¥çš„ãªæ‹¡å¼µï¼‰
        # ç¾æ®µéšã§ã¯ã€ç™ºè¦‹ã—ãŸæƒ…å ±ã®ä»¶æ•°ã‚’è¦ç´„ã¨ã™ã‚‹
        summary_text = (
            f"åˆè¨ˆ {len(all_findings)} ä»¶ã®é–¢é€£æƒ…å ±ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚\n"
            f"å†…è¨³: å†…éƒ¨æ–‡æ›¸ {len(internal_findings)} ä»¶, ã‚¦ã‚§ãƒ– {len(web_findings)} ä»¶ã€‚"
        )
        report = schemas.ResearchReport(findings=all_findings, summary=summary_text)

    # 4. ã‚¹ãƒ©ã‚¤ãƒ‰ã”ã¨ã®å†…éƒ¨è¦ç´„ã‚’ä½œæˆ
    slide_summaries: Dict[str, str] = {}
    blueprints = state.get("slide_blueprints", [])
    for bp in blueprints:
        sq = getattr(bp, "search_query", None) or f"{bp.slide_title} {query}"
        bp_findings = markdown_search_tool.search(sq, top_k=3)
        if not bp_findings:
            slide_summaries[bp.slide_id] = "é–¢é€£ã™ã‚‹è¦ç´„ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            continue
        # ç°¡æ˜“è¦ç´„: é€£çµâ†’å¥ç‚¹ã§åˆ†å‰²â†’ã‚¹ã‚³ã‚¢ä»˜ã‘â†’ä¸Šä½3æ–‡ã‚’å†æ§‹æˆ
        joined = " ".join([f.content for f in bp_findings])
        sentences = [s.strip() for s in re.split(r"[ã€‚\.]", joined) if len(s.strip()) > 10]
        # ã‚¹ã‚³ã‚¢: åè©ã‚‰ã—ããƒˆãƒ¼ã‚¯ãƒ³é »åº¦ã®åˆè¨ˆ
        def tokenize(t: str) -> List[str]:
            return [w for w in re.split(r"[\sã€ï¼Œ,ãƒ»:ï¼š;ï¼›()ï¼ˆï¼‰\-]+", t) if len(w) >= 2]
        freq: Dict[str, int] = {}
        for s in sentences:
            for w in tokenize(s):
                freq[w] = freq.get(w, 0) + 1
        scored = []
        for s in sentences:
            score = sum(freq.get(w, 0) for w in tokenize(s))
            scored.append((score, s))
        scored.sort(reverse=True)
        top = [s for _, s in scored[:3]]
        if not top:
            slide_summaries[bp.slide_id] = "é–¢é€£ã™ã‚‹è¦ç´„ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        else:
            # 2-3æ–‡ã§çµ±åˆè¦ç´„
            y = []
            for s in top:
                s = re.sub(r"(ã¨ã„ã£ãŸ|ãªã©).*", "ãªã©ã‚’æ•´ç†ã€‚", s)
                y.append(s)
            slide_summaries[bp.slide_id] = "ã€‚".join(y)[:300]

    print("--- âœ… ãƒªã‚µãƒ¼ãƒãƒ¬ãƒãƒ¼ãƒˆãŒå®Œæˆã—ã¾ã—ãŸ ---")
    return {"research_report": report, "slide_summaries": slide_summaries}

